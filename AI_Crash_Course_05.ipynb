{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-Crash_Course_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGKc6cNXXcWVB710HbFzGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHern/AI-Crash-Course/blob/main/AI_Crash_Course_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3L6p00dGZs4"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "[Presentation: AI Crash Course 05](https://view.genial.ly/619e85e4ae59f30d5cb303d0/presentation-ai-crashcourse05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hOe4K0Hmut"
      },
      "source": [
        "## 0) Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWUjG0IIJkk"
      },
      "source": [
        "# ----- Libraries ----- #\n",
        "\n",
        "# For graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# For visualizing more complex graphs\n",
        "import seaborn as sns\n",
        "\n",
        "# Neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global constant for training acceleration\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4qW8UlP5LQ"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okEGIBRLLg1z"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncn_CmxWYqL1"
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "raw_dataset = pd.read_csv(\"diabetes.csv\")\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I53Qd8obhFg"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2yDVbIpblVZ"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8y5U8Tbnqf"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19wGecl7bpJd"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "# df.dropna()\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwagrZYbpp2"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QchVy_bq_t"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = list(raw_dataset.columns)\n",
        "independent_variables.remove('Outcome')\n",
        "dependent_variables = ['Outcome']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTY6_xH2-2eh"
      },
      "source": [
        "## 2) Callbacks Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6czjL55-5Lm"
      },
      "source": [
        "'''\n",
        "  CALLBACKS:\n",
        "  These are functions that are called once a certain amount of epochs end. These come very handful to deal with overfitting\n",
        "'''\n",
        "\n",
        "# Lets load an extension that helps us to visualize the performance of our model\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Lets create the callback for that\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/model/', histogram_freq=1)\n",
        "\n",
        "# Now lets create a callback that save our model every epoch\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('logs/model/', monitor='accuracy', verbose=1, save_best_only=True, mode='max',save_freq='epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NmOn_vecR3"
      },
      "source": [
        "## 3) Underfit NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17v9Qk4xcIPi"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=2, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=5,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[tensorboard_callback, checkpoint_callback]\n",
        "        )\n",
        "\n",
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhQNj6_TBF1h"
      },
      "source": [
        "# Lets visualize\n",
        "%tensorboard --logdir \"logs/model/\" --port=8008"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STx8FeovBuTB"
      },
      "source": [
        "## 4) Overfit NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pctpAoDNBtyo"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=50,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[tensorboard_callback, checkpoint_callback]\n",
        "        )\n",
        "\n",
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjC4gm0bCQve"
      },
      "source": [
        "# Lets visualize\n",
        "%tensorboard --logdir \"logs/model/\" --port=8008"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8m0DT_xDjhk"
      },
      "source": [
        "## 5) More balanced and accepted NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bxNtOUdDlvz"
      },
      "source": [
        "'''\n",
        "  Lets introduce a new callback to counter overfitting: EarlyStopping\n",
        "  This callback stop the training once a criteria has met. This prevents overfitting.\n",
        "  Important parameters:\n",
        "  1. monitor: the metric which the callback does monitor. Validation loss is the accepted metric to observer\n",
        "  2. Min Delta: This is the threshold that the callback uses to tell if the model has improved or not.\n",
        "  This is the difference that the metric should satisfy between epochs.\n",
        "  3. Patience: This is how many epochs does the callback has to see the model has not improved and immediately stops\n",
        "  training afterwards\n",
        "'''\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqCBv470DqPW"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=100,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[tensorboard_callback, checkpoint_callback, earlystopping_callback]\n",
        "        )\n",
        "\n",
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRUXelS5E7ig"
      },
      "source": [
        "# Lets visualize\n",
        "%tensorboard --logdir \"logs/model/\" --port=8008"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}