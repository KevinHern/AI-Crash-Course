{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-Crash_Course_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPmWToqsPycIYFDufa3+sCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinHern/AI-Crash-Course/blob/main/AI_Crash_Course_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3L6p00dGZs4"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "[Presentation: AI Crash Course 04](https://view.genial.ly/619d2404371f850d8bb13f7e/presentation-ai-crashcourse04)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hOe4K0Hmut"
      },
      "source": [
        "## 0) Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWUjG0IIJkk"
      },
      "source": [
        "# ----- Libraries ----- #\n",
        "\n",
        "# For graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# For visualizing more complex graphs\n",
        "import seaborn as sns\n",
        "\n",
        "# Neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global constant for training acceleration\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE_cmElZJ4PE"
      },
      "source": [
        "# Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARASHphaKU6D"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMmvqGMGOXu"
      },
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgXcpexoIG9P"
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model_Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"auto-mpg.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W3AsBCPQMHu"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYYb14mKiz6"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8kp61PKzl5"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taL7hSd9LHpj"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-wSRqvEPoLo"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[[\"Cylinders\", \"Displacement\", \"Weight\", \"Acceleration\", \"Horsepower\", \"MPG\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKsREMcO91a"
      },
      "source": [
        "## 2) NN for a simple Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xolCD5p4QFa8"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['Cylinders','Displacement', 'Horsepower','Weight', 'Acceleration', 'Model_Year', 'Origin']\n",
        "dependent_variables = ['MPG']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efOETJ7cPkMO"
      },
      "source": [
        "# Lets build a simple model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1tt040L5d0"
      },
      "source": [
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=5,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          #callbacks=callbacks_list\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBGnFIGtJ_9e"
      },
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l31mWppGPKRf"
      },
      "source": [
        "## 3) NN for multiple Regressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faCY4iaPPgl"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['Cylinders','Displacement', 'Weight' ,'Acceleration', 'Model_Year', 'Origin']\n",
        "dependent_variables = ['MPG', 'Horsepower']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRBSUiPqPWDl"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=20,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          #callbacks=callbacks_list\n",
        "        )\n",
        "\n",
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4qW8UlP5LQ"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okEGIBRLLg1z"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncn_CmxWYqL1"
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "raw_dataset = pd.read_csv(\"diabetes.csv\")\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I53Qd8obhFg"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2yDVbIpblVZ"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8y5U8Tbnqf"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19wGecl7bpJd"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "# df.dropna()\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwagrZYbpp2"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QchVy_bq_t"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = list(raw_dataset.columns)\n",
        "independent_variables.remove('Outcome')\n",
        "dependent_variables = ['Outcome']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NmOn_vecR3"
      },
      "source": [
        "## 2) NN for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17v9Qk4xcIPi"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=25,\n",
        "          batch_size = 128 ,\n",
        "          validation_split=0.2,\n",
        "          #callbacks=callbacks_list\n",
        "        )\n",
        "\n",
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aTmhnXieaFe"
      },
      "source": [
        "# Function that plots confusion matrix\n",
        "def plot_confusion_matrix(labels, predictions):\n",
        "  figure = plt.figure(figsize=(4, 4))\n",
        "  sns.heatmap(confusion_matrix(labels=labels, predictions=predictions), annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "predictions = list(map(lambda x: 1 if x > 0.5 else 0, model.predict(test_set)))\n",
        "\n",
        "plot_confusion_matrix(labels=test_target, predictions=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3DicqEjZAu"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO7kBZ-MPLaP"
      },
      "source": [
        "## 1) Data Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAXYOI9RZxxM"
      },
      "source": [
        "import itertools \n",
        "from keras.preprocessing import image\n",
        "\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "img_rows = [(i-(IMG_WIDTH/2))/(IMG_WIDTH/2) for i in range(IMG_WIDTH)]\n",
        "img_cols = [(j-(IMG_HEIGHT/2))/(IMG_HEIGHT/2) for j in range(IMG_HEIGHT)]\n",
        "\n",
        "flatten_image = np.array(list(itertools.product(img_rows, img_cols)))"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0MNfvfr5cQ"
      },
      "source": [
        "## 2) NN for Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7hMmVEr8LU"
      },
      "source": [
        "# Creating a custom Layer\n",
        "class ScaleLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, redScale=255.0, greenScale=255.0, blueScale=255.0):\n",
        "    super(ScaleLayer, self).__init__()\n",
        "    self.scale = tf.constant([redScale, greenScale, blueScale], dtype=tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.dtypes.cast(inputs, tf.float32)\n",
        "    outputs = outputs * self.scale\n",
        "    return tf.dtypes.cast(outputs, tf.uint8)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BrfvIysFzD"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(2, )),\n",
        "  tf.keras.layers.Dense(units=128, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=256, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=512, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=3, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal()),\n",
        "  ScaleLayer(redScale=100, greenScale=0, blueScale=255)\n",
        "  ])\n",
        "generated_image = np.reshape(np.array(model(flatten_image)), newshape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image.array_to_img(generated_image))\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}